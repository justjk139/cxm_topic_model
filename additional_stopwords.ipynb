{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jihok/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk; nltk.download('stopwords')\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cells prior to adding additional stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS APPLY CURRENT CLEANING STEPS INTO ENTIRE DATASET\n",
    "\n",
    "def subject_clean(subject_line):\n",
    "    regex_sol = re.sub(r'https?://\\S+', '', subject_line) # removes URL links\n",
    "    regex_sol = re.sub(r\"\\S*@\\S*\\s?\", \"\", regex_sol) # removes email accounts\n",
    "    regex_sol = regex_sol.replace(\"\\n\", \"\").replace(\"< >\",\"\").replace(\"\\r\", \"\") # removes newline and <> and \\r\n",
    "    regex_sol = re.sub(r\"\\d+\", \"\", regex_sol) # removes integers \n",
    "    regex_sol = re.sub(r\"([^\\s\\w]|_)+\", \" \", regex_sol) # removes non-alphanumeric characters, but maintains whitespace\n",
    "    regex_sol = regex_sol.encode(\"ASCII\", \"replace\").decode(\"utf-8\").replace(\"?\", \" \") # removes all non-ASCII characters\n",
    "    regex_sol = regex_sol.lower() # lower case string\n",
    "    return regex_sol\n",
    "\n",
    "def case_line_clean(case_line):\n",
    "    regex_sol_2 = re.sub(r\"[?](CS)[0-9]+\", \"\", case_line) # removes ending \"?CS###\"\n",
    "    regex_sol_2 = re.sub(r\"\\d+\", \"\", regex_sol_2) # removes integers\n",
    "    regex_sol_2 = re.sub(r\"([^\\s\\w]|_)+\", \" \", regex_sol_2) # removes non-characters, but maintains whitespace\n",
    "    regex_sol_2 = regex_sol_2.lower()\n",
    "    return regex_sol_2\n",
    "\n",
    "# call the dataset that pertains to you\n",
    "august_dataset = pd.read_csv(\"SNOW August Data.csv\", encoding = \"ISO-8859-1\")\n",
    "september_dataset = pd.read_csv(\"SNOW September Data.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "#SPAM case removal\n",
    "august_dataset = august_dataset.loc[(august_dataset['contact'] != 'Spam spam') \n",
    "                                    & (august_dataset['contact'] != 'SPAM SPAM') \n",
    "                                    & (august_dataset['resolution_code'] != 'Spam') \n",
    "                                    & (august_dataset['service_offering'] == 'Other Buying Inquiry')].reset_index(drop=True)\n",
    "\n",
    "september_dataset = september_dataset.loc[(september_dataset['contact'] != 'Spam spam') \n",
    "                                          & (september_dataset['contact'] != 'SPAM SPAM') \n",
    "                                          & (september_dataset['resolution_code'] != 'Spam') \n",
    "                                          & (september_dataset['service_offering'] == 'Other Buying Inquiry')].reset_index(drop=True)\n",
    "\n",
    "# Regex cleaning applied to both the \"description\" and \"case\" columns in each dataset\n",
    "august_dataset['description'] = august_dataset['description'].apply(subject_clean)\n",
    "september_dataset['description'] = september_dataset['description'].apply(subject_clean)\n",
    "\n",
    "august_dataset['case'] = august_dataset['case'].apply(case_line_clean)\n",
    "september_dataset['case'] = september_dataset['case'].apply(case_line_clean)\n",
    "\n",
    "# print(len(august_dataset)) 421\n",
    "# print(len(september_dataset)) 210\n",
    "\n",
    "# Concat the cleaned data into one dataframe for use\n",
    "# June annd July have been removed \n",
    "all_months = pd.concat([august_dataset, september_dataset]).reset_index()\n",
    "# len(all_months) 631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes cases where \"wav file\" and \"idt\" exists\n",
    "# Current iteration removes 85 rows from all_months dataset\n",
    "delete_rows = []\n",
    "for i in range(len(all_months)):\n",
    "    curr = all_months.iloc[i]['description']\n",
    "    test1 = re.findall(r\"\\b(wav.file)\\b\", curr)\n",
    "    test2 = re.findall(r\"\\b(idt)\\b\", curr)\n",
    "    if len(test1) > 0 or len(test2) > 0:\n",
    "        delete_rows.append(i)\n",
    "\n",
    "all_months = all_months.drop(delete_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Additional stopwords\n",
    "domain_stop_words = [\n",
    "    'hi', 'hello', 'thank', 'thanks', 'com', 'the', 're', 'php', 'http', 'XXXXXXXXX', 'would', 'however', \n",
    "    'please', 'do', 'can', 'may', 'check', 'pende', 'tell', 'use', 'call', 'let', 'dear', 'see', 'click', \n",
    "    'still', 'unable', 'even', 'minute', 'basically', 'seem', 'expect', 'pcie', 'usd', 'go', 'could', \n",
    "    'advise', 'appreciate', 'regard', 'also', 'end', 'sure', 'copy', 'phone', 'know', 'accidently', \n",
    "    'reply', 'web', 'soon', 'regard', 'get', 'try', 'new', 'follow', 'date', 'pm' ,'back', 'note', 'us', \n",
    "    'sku', 'sincerely', 'immediately', 'notify', 'one', 'two', 'someone', 'day', 'put', 'start', 'set', \n",
    "    'reply', 'advise', 'august', 'arise', 'therewith', 'regarding', 'san', 'diego', 'uc', 'inc', 'ca',\n",
    "    'go', 'able', 'say', 'like', 'wav_file', 'ref', 'monday', 'marketplace', 'try', 'time', 'use','want',\n",
    "    'ucsd','pur', 'support', 'provide', 'question','darmstadt', 'germany', 'accept', 'liability','office',\n",
    "    'subject', 'email','sent','confidential','attachment','say','pdf','sender', 'comments','v', 'customer',\n",
    "    'services', 'abcam','kendall','square','suite','cambridge', 'usatoll','free','international','tel','fax',\n",
    "    'hours','est','mon','frigoods','duties','unpaid','control','placing','agreeing','duties','applicable', \n",
    "    'wondering', 'happens', 'something', 'janelle', 'chartstream', 'needs', 'believe', 'attached', 'cce', \n",
    "    'zhu', 'rm', 'ste', 'cd', 'going', 'received', 'much', 'kind', 'regards', 'drydock', 'avenueboston', \n",
    "    'linethank', 'ab', 'from'\n",
    "    ]      \n",
    "stop_words.extend(domain_stop_words)\n",
    "# stop_words.extend(domain_stop_words + curr_list + more_stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing description into individual words aka tokenizing\n",
    "alldescrip = list(all_months['description'])\n",
    "\n",
    "#initiate blank list\n",
    "text_words = []\n",
    "\n",
    "#for loop to tokenize all words in\n",
    "for sentence in alldescrip:\n",
    "    temp = gensim.utils.simple_preprocess(sentence)\n",
    "    text_words.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram\n",
    "bigram = gensim.models.Phrases(text_words, min_count=5, threshold=100)\n",
    "\n",
    "#Removeal of stop words\n",
    "text_words_no_stops = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in text_words]\n",
    "\n",
    "#Make texts into bigram model\n",
    "text_words_no_stops_bigram = [bigram[doc] for doc in text_words_no_stops]\n",
    "\n",
    "\n",
    "data_words = [item for sublist in text_words_no_stops_bigram for item in sublist]\n",
    "\n",
    "# remove stop words first before applying\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(data_words)\n",
    "\n",
    "# uncomment line below to see list of word frequencies\n",
    "# fdist.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Adding Additional Stopwords Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual additional stopwords\n",
    "manual_stop_words = [\n",
    "    'team', 'details','possible','priviledged', 'milliporesigma', 'merck','kgaa', 'must', \n",
    "    'disclose', 'disclaimer','spanish', 'german_french', 'portuguese','versions', \n",
    "    'contacting', 'milliporesigma','name','dr','item','pmto','list','center','items',\n",
    "    'style','aug','via','usa','today', 'original','thereto','merck','best','california',\n",
    "    'days','using','rights, reserved','miltenyi','biotec','amto','llc','wrote','link',\n",
    "    'way','airgas','px','bu','msd','qiagen','actual','well','care','ml','already','color',\n",
    "    'wed','th','colleen','thermo, fisher','style', 'font','aug','ea','addgene','mrs',\n",
    "    'gilman','asap','looking','www','br','br','size','px','shclng','trcn','ph','cc','per',\n",
    "    'non','ltd'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional stopwords added here\n",
    "add_stopwords = []\n",
    "for i in fdist.most_common():\n",
    "    # add/adjust conditionals here\n",
    "    if i[1] < 5:\n",
    "        add_stopwords.append(i[0])\n",
    "print(len(add_stopwords))\n",
    "add_stopwords\n",
    "\n",
    "conditional_stopwords = []\n",
    "for i in add_stopwords:\n",
    "    if \"_\" in i:\n",
    "        curr = i.split(\"_\")\n",
    "        for x in curr:\n",
    "            conditional_stopwords.append(x)\n",
    "    else:\n",
    "        conditional_stopwords.append(i)\n",
    "conditional_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3620\n"
     ]
    }
   ],
   "source": [
    "# stop_words_blk_1 is comprised of both the manual and conditional list of stopwords\n",
    "stop_words_blk_1 = conditional_stopwords + manual_stop_words\n",
    "print(len(stop_words_blk_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'stop_words_blk_1' (list)\n"
     ]
    }
   ],
   "source": [
    "# stores variable in IPython db to allow other notebooks access to the variable\n",
    "# Run after any new words are added\n",
    "%store stop_words_blk_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to remove stored variable \n",
    "# %store -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "02e2b76c3f36b561b545fd33b215e899359830b31734d753d43a9ffe1154bc22"
    }
   },
   "name": "Python 3.7.9 64-bit ('anaconda3': virtualenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
